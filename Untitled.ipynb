{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3069119a-7ca4-4a94-b664-ad6e6a81e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\saite\\AppData\\Local\\Temp\\ipykernel_14688\\616159744.py:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  file_path = \"D:\\CreaditApproval\\CreaditApproval\\creditcard project final.ipynb\"  # Replace 'your_notebook.ipynb' with your actual notebook file path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
      "\n",
      "#credit_data==='x'\n",
      "#application_data==='y'\n",
      "cd C:\\Users\\DELL\\Downloads\\Final one Hack\\minor proj\\minor proj\\CreaditApproval\n",
      "\n",
      "x=pd.read_csv(\"credit_record.csv\")\n",
      "x.head()\n",
      "x.tail()\n",
      "x.index\n",
      "x.info()\n",
      "x.describe(percentiles=[0.5,0.65,1.0])\n",
      "x.nunique()\n",
      "x.duplicated()\n",
      "x.count()\n",
      "x.STATUS.count()\n",
      "<h2>What is the distribution of credit statuses (e.g., \"0\" - paid off, \"1\" - one month delay, etc.)?\n",
      "</h2>\n",
      "x\n",
      "status=x['STATUS'].value_counts()\n",
      "status                           #each credit card status\n",
      "#\"0\" - Paid off: 383120 occurrences\n",
      "#\"1\" - One month delay: 11090 occurrences\n",
      "#\"2\" - Two months delay: 868 occurrences\n",
      "#\"3\" - Three months delay: 320 occurrences\n",
      "#\"C\" - Closed accounts: 442031 occurrences\n",
      "#\"X\" - Unknown or unspecified status: 209230 occurrences\n",
      "<h2>How many unique individuals are represented in the dataset?</h2>\n",
      "individuals=x['ID'].nunique()\n",
      "individuals   \n",
      "<h2>What is the average number of months for which credit records are available per individual?</h2>\n",
      "indi=x.groupby('ID')[\"MONTHS_BALANCE\"].nunique()\n",
      "indi\n",
      "avg=indi.mean()\n",
      "avg\n",
      "x[\"STATUS\"]\n",
      "<h2>What is the most common credit status for each individual?</h2>\n",
      "cmn_cr_sta=x[\"STATUS\"].max()\n",
      "cmn_cr_sta\n",
      "mst_cmn_st=x.groupby('ID')['STATUS'].apply(lambda i: i.value_counts().idxmax())\n",
      "mst_cmn_st\n",
      "<h2>How many individuals have experienced a significant increase in credit delays over time?\n",
      "\n",
      "</h2>\n",
      "min_months_balance=x.groupby('ID')['MONTHS_BALANCE'].min()\n",
      "print(min_months_balance)\n",
      "max_months_balance=x.groupby('ID')['MONTHS_BALANCE'].max()\n",
      "print(max_months_balance)\n",
      "months_balance_difference = max_months_balance-min_months_balance\n",
      "print(months_balance_difference)\n",
      "significant_increase_individuals = months_balance_difference[months_balance_difference > 0]\n",
      "significant_increase_individuals\n",
      "num_significant_increase_individuals = len(significant_increase_individuals)\n",
      "print(\"Number of individuals with a significant increase in credit delays:\", num_significant_increase_individuals)\n",
      "<h2>Is there a correlation between the duration of credit history and the credit status?\n",
      "\n",
      "</h2>\n",
      "# Calculate the duration of credit history for each individual\n",
      "x['DURATION']=x.groupby('ID')['MONTHS_BALANCE'].transform(lambda i: i.nunique())\n",
      "x[\"DURATION\"]\n",
      "# Filter out non-numeric credit statuses\n",
      "numeric_statuses = x['STATUS'].str.isnumeric()\n",
      "x_filtered = x[numeric_statuses]\n",
      "x_filtered\n",
      "x_filtered['STATUS'] = x_filtered['STATUS'].astype(int)\n",
      "average_credit_status_by_duration = x_filtered.groupby('DURATION')['STATUS'].mean()\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.plot(average_credit_status_by_duration.index, average_credit_status_by_duration.values, marker='o')\n",
      "plt.xlabel('Duration of Credit History (Months)')\n",
      "plt.ylabel('Average Credit Status')\n",
      "plt.title('Average Credit Status by Duration of Credit History')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "<h2>How many months of credit history does the dataset cover?\n",
      "\n",
      "\n",
      "</h2>\n",
      "min_month=x['MONTHS_BALANCE'].min()\n",
      "max_month=x['MONTHS_BALANCE'].max()\n",
      "num_months_credit_history=max_month-min_month+1\n",
      "print(\"Number of months of credit history covered by the dataset:\", num_months_credit_history)\n",
      "<h2>What is the average credit delay across all individuals in the dataset?\n",
      "\n",
      "</h2>\n",
      "average_credit_delay=x[x['STATUS'].isin(['0', '1', '2', '3', '4', '5'])]['STATUS'].astype(int).mean()\n",
      "average_credit_delay\n",
      "<h2>What is the most common credit status across all individuals?\n",
      "</h2>\n",
      "credit_status_counts = x['STATUS'].value_counts()\n",
      "most_common_credit_status = credit_status_counts.idxmax()\n",
      "most_common_credit_status\n",
      "<h2>What is the maximum number of credit delays experienced by an individual in the dataset?\n",
      "\n",
      "</h2>\n",
      "numeric_statuses = x['STATUS'].str.isnumeric()\n",
      "x_filtered = x[numeric_statuses]\n",
      "x_filtered['STATUS'] = x_filtered['STATUS'].astype(int)\n",
      "max_credit_delay_per_individual = x_filtered.groupby('ID')['STATUS'].max()\n",
      "max_credit_delay = max_credit_delay_per_individual.max()\n",
      "max_credit_delay\n",
      "cd C:\\Users\\hp\\Desktop\\sai folders\\Sai Teja Thatikonda\\data science projects\\datasets\\Practice Dataset\\CreaditApproval\n",
      "y=pd.read_csv(\"application_record.csv\")\n",
      "y.head()\n",
      "y.describe()\n",
      "y.describe(percentiles=[0.125,0.5,0.65,0.75])\n",
      "y.info()\n",
      "y.isnull().sum()\n",
      "y.notnull().sum()\n",
      "y.shape\n",
      "y.size\n",
      "y.duplicated()\n",
      "y.dropna()\n",
      "y.columns\n",
      "y.describe(include=object)\n",
      "# Get the distinct values of each column\n",
      "for column in y.columns:\n",
      "    unique_val=y[column].unique()\n",
      "    print(\"unique values of {} are {}\".format(column,unique_val))\n",
      "y.head()\n",
      "# Get the distinct values of specified columns\n",
      "spec_cols = ['NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE']\n",
      "for col in spec_cols:\n",
      "    uni_val=y[col].unique()\n",
      "    print(\"----------------------------\")\n",
      "    print(col)\n",
      "    print(\"----------------------------\")\n",
      "    for value in uni_val:\n",
      "        print(value)\n",
      "#Filtering rows based on the presence of null values using the dropna() method:\n",
      "filtered_dropna = y.dropna(axis=0) # default axis value is zero ==> rows with missing values are deleted\n",
      "print(\"Total records Before:\" , y.shape)\n",
      "print(\"Total records After:\" , filtered_dropna.shape)\n",
      "#Filtering rows based on the presence of null values using the dropna() method:\n",
      "\n",
      "filtered_dropna_2 = y.dropna(axis=1) # ==> columns with missing values are deleted\n",
      "\n",
      "print(\"Total records Before:\" , y.shape)\n",
      "print(\"Total records After:\" , filtered_dropna_2.shape)\n",
      "dup_test = y[y.duplicated()]\n",
      "dup_test.shape\n",
      "# Filter rows based on condition'AMT_INCOME_TOTAL' >= 250000\n",
      "filtered_income_data = y[y['AMT_INCOME_TOTAL'] >=250000 ]\n",
      "print(\"Total records:\", len(filtered_income_data))\n",
      "filtered_income_data.head()\n",
      "# Using where condition\n",
      "filtered_income_data2 = y.where(y['AMT_INCOME_TOTAL'] >= 250000)\n",
      "print(\"Total records:\", len(filtered_income_data2))\n",
      "filtered_income_data2.head()\n",
      "# Filter rows where 'CODE_GENDER' is 'F'\n",
      "filtered_female_data = y[y['CODE_GENDER'] == 'F' ]\n",
      "\n",
      "print(\"Total records:\", len(filtered_female_data))\n",
      "\n",
      "filtered_female_data.head()\n",
      "# Filter rows where 'NAME_FAMILY_STATUS' is NOT 'Civil marriage', 'Married'\n",
      "fam_status=y[~y[\"NAME_FAMILY_STATUS\"].isin([\"Civil marriage\",\"Married\"])]\n",
      "fam_status\n",
      "# Filter rows where 'NAME_INCOME_TYPE' startswith 'Working'\n",
      "name_inc=y[y[\"NAME_INCOME_TYPE\"].str.startswith(\"Working\")]\n",
      "name_inc\n",
      "# Filering using \"query\" and \"isin\" filter male_car_owner where CODE_GENDER is M and FLAG_OWN_CAR in Y\n",
      "male_car_owner=y.query('CODE_GENDER.isin([\"M\"]) & FLAG_OWN_CAR.isin([\"Y\"])')\n",
      "male_car_owner\n",
      "# Filering without isin\n",
      "male_car_owners2=y.query('CODE_GENDER==\"M\" & FLAG_OWN_CAR==\"Y\"')\n",
      "male_car_owners2\n",
      "# Filering using \"loc\" and columns range using \":\"  ID TO CNT_CHILDREN\n",
      "cols=y.loc[:,'ID':'CNT_CHILDREN']\n",
      "cols\n",
      "#onlu 5 rows\n",
      "\n",
      "cols2=y.loc[0:4,'ID':'CNT_CHILDREN']\n",
      "cols2\n",
      "#all columns\n",
      "cols3=y.loc[0:6, :]\n",
      "cols3\n",
      "# Filering specific columns\n",
      "\n",
      "subset_data = y[['ID', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALITY', 'CNT_CHILDREN']]\n",
      "subset_data\n",
      "#filter male car owners and col range from 0 to 4\n",
      "male_car_owner3=y.loc[(y[\"CODE_GENDER\"]==\"M\") & (y[\"FLAG_OWN_CAR\"]==\"Y\")].iloc[:,0:6]\n",
      "print(len(male_car_owner3))\n",
      "male_car_owner3.head()\n",
      "#filter using lambda\n",
      "male_car_owner4=y[y.apply(lambda i:i['CODE_GENDER']==\"M\" and 'Y' in i[\"FLAG_OWN_CAR\"],axis=1)]\n",
      "male_car_owner4\n",
      "# Accessing specific rows and columns using iloc\n",
      "# Select the first three rows and the first two columns\n",
      "rows_and_cols=y.iloc[0:3,0:2]\n",
      "rows_and_cols\n",
      "#unique family status\n",
      "y[\"NAME_FAMILY_STATUS\"]\n",
      "unique_family_status=y[~y[\"NAME_FAMILY_STATUS\"].duplicated()].loc[:,[\"NAME_FAMILY_STATUS\"]]\n",
      "unique_family_status\n",
      "#top 10 income data\n",
      "income_data=y[y[\"AMT_INCOME_TOTAL\"].rank(ascending=False)<=10].loc[:,'ID':'AMT_INCOME_TOTAL']\n",
      "income_data\n",
      "\n",
      "y.dtypes\n",
      "filterd_num=y.select_dtypes(include='int').iloc[0:5,:]\n",
      "filterd_num\n",
      "filterd_num=y.select_dtypes(include='number').iloc[0:5,:]\n",
      "filterd_num\n",
      "filterd_obj=y.select_dtypes(include='object').iloc[0:4,:]\n",
      "filterd_obj\n",
      "filterd_obj=y.select_dtypes(include='float').iloc[0:4,:]\n",
      "filterd_obj\n",
      "# Create new variable : age_in_years using DAYS_BIRTH\n",
      "y[\"age_in_years\"]=-round(y['DAYS_BIRTH']/365,0)\n",
      "y.head()\n",
      "#now delete DAYS_BIRTH column\n",
      "y.drop('DAYS_BIRTH', axis=1,inplace=True)\n",
      "y.head()\n",
      "# Create new variable : EXP_IN_YEARS using DAYS_EMPLOYED\n",
      "y['EXP_IN_YEARS']=-round(y['DAYS_EMPLOYED']/365,0)\n",
      "y.head()\n",
      "#replace varibles with 0 and 1\n",
      "y['CODE_GENDER']=y['CODE_GENDER'].replace(['F','M'],[0,1]) #0->F 1->M\n",
      "y['FLAG_OWN_CAR']=y['FLAG_OWN_CAR'].replace(['N','Y'],[0,1]) #0->N 1->Y\n",
      "y['FLAG_OWN_REALTY']=y['FLAG_OWN_REALITY'].replace(['N','Y'],[0,1])\n",
      "y.head()\n",
      "y=y.dropna(axis=0)\n",
      "print(y.shape)\n",
      "#VISUALIZATION\n",
      "x\n",
      "y\n",
      "y.head()\n",
      "merged_data = y.merge(x, on='ID', how='inner')\n",
      "merged_data\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, mean_squared_error, classification_report\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "\n",
      "target_mapping = {\n",
      "    \"0\": 1,  # Approved\n",
      "    \"1\": 1,  # Approved\n",
      "    \"2\": 1,  # Approved\n",
      "    \"3\": 1,  # Approved\n",
      "    \"C\": 0,  # Denied\n",
      "    \"X\": 0   # Denied\n",
      "}\n",
      "x['target'] = x['STATUS'].map(target_mapping)\n",
      "a = merged_data.drop('STATUS', axis=1)\n",
      "b = merged_data['STATUS']\n",
      "x.head()\n",
      "y\n",
      "y.head()\n",
      "import pandas as pd\n",
      "\n",
      "# Merge datasets based on the 'ID' column\n",
      "merged_data = x.merge(y, on='ID', how='inner')\n",
      "\n",
      "merged_data\n",
      "status_mapping = {\n",
      "    \"0\": 1,  # Approved\n",
      "    \"1\": 1,  # Approved\n",
      "    \"2\": 1,  # Approved\n",
      "    \"3\": 1,  # Approved\n",
      "    \"C\": 0,  # Denied\n",
      "    \"X\": 0   # Denied\n",
      "}\n",
      "\n",
      "# Use the map function to apply the mapping to the STATUS column\n",
      "merged_data['STATUS'] = merged_data['STATUS'].map(status_mapping)\n",
      "\n",
      "merged_data['STATUS']\n",
      "merged_data\n",
      "z=pd.read_csv(\"application_record.csv\")\n",
      "merged_data['NAME_INCOME_TYPE'] = z['NAME_INCOME_TYPE']\n",
      "merged_data\n",
      "merged_data=merged_data.dropna()\n",
      "merged_data\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "label_encoder = LabelEncoder()\n",
      "merged_data['OCCUPATION_TYPE'] = label_encoder.fit_transform(merged_data['OCCUPATION_TYPE'])\n",
      "merged_data['NAME_HOUSING_TYPE'] = label_encoder.fit_transform(merged_data['NAME_HOUSING_TYPE'])\n",
      "merged_data['STATUS'] = label_encoder.fit_transform(merged_data['STATUS'])\n",
      "merged_data['NAME_INCOME_TYPE'] = label_encoder.fit_transform(merged_data['NAME_INCOME_TYPE'])\n",
      "merged_data['NAME_EDUCATION_TYPE'] = label_encoder.fit_transform(merged_data['NAME_EDUCATION_TYPE'])\n",
      "merged_data['NAME_FAMILY_STATUS'] = label_encoder.fit_transform(merged_data['NAME_FAMILY_STATUS'])\n",
      "merged_data['FLAG_OWN_REALITY'] = label_encoder.fit_transform(merged_data['FLAG_OWN_REALITY'])\n",
      "#logistic regression\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, mean_squared_error, classification_report\n",
      "\n",
      "# features (X) and target variable (y)\n",
      "X = merged_data.drop('target', axis=1)  #  'target' is the name of our binary target variable\n",
      "y = merged_data['target']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "y_pred = model.predict(X_test)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "f1 = f1_score(y_test, y_pred)\n",
      "precision = precision_score(y_test, y_pred)\n",
      "recall = recall_score(y_test, y_pred)\n",
      "roc_auc = roc_auc_score(y_test, y_pred)\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "classification_rep = classification_report(y_test, y_pred)\n",
      "print(f\"Accuracy: {accuracy}\")   \n",
      "print(f\"F1 Score: {f1}\")        \n",
      "print(f\"Precision: {precision}\")  \n",
      "print(f\"Recall: {recall}\")       \n",
      "print(f\"ROC AUC: {roc_auc}\")     #discriminate blw pos and neg classes across diff threshold values \n",
      "print(f\"Mean Squared Error: {mse}\")\n",
      "print(\"Classification Report:\\n\", classification_rep)\n",
      "\n",
      "#decision trees\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "X = merged_data.drop(columns=['STATUS'])\n",
      "y = merged_data['STATUS']\n",
      "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
      "tree_classifier.fit(X_train, y_train)\n",
      "y_pred = tree_classifier.predict(X_test)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(f\"Accuracy: {accuracy}\")\n",
      "classification_rep = classification_report(y_test, y_pred)\n",
      "print(\"Classification Report:\\n\", classification_rep)\n",
      "\n",
      "#random forest\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "X = merged_data.drop(columns=['STATUS'])\n",
      "y = merged_data['STATUS']\n",
      "\n",
      "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_classifier.fit(X_train, y_train)\n",
      "y_pred = rf_classifier.predict(X_test)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(f\"Accuracy: {accuracy}\")\n",
      "classification_rep = classification_report(y_test, y_pred)\n",
      "print(\"Classification Report:\\n\", classification_rep)\n",
      "# missing data in the dataset\n",
      "missing_data = merged_data.isnull().sum()\n",
      "print(\"Missing Data:\")\n",
      "print(missing_data)\n",
      "\n",
      "#outlier\n",
      "columns_to_check = ['AMT_INCOME_TOTAL', 'STATUS']\n",
      "z_scores = np.abs((merged_data[columns_to_check] - merged_data[columns_to_check].mean()) / merged_data[columns_to_check].std())\n",
      "threshold = 3\n",
      "outliers = (z_scores > threshold).any(axis=1)\n",
      "outlier_data = merged_data[outliers]\n",
      "\n",
      "print(\"Outliers:\")\n",
      "print(outlier_data)\n",
      "\n",
      "# #svm\n",
      "# import numpy as np\n",
      "# import pandas as pd\n",
      "# from sklearn.model_selection import train_test_split\n",
      "# from sklearn.svm import SVC\n",
      "# from sklearn.metrics import accuracy_score, classification_report\n",
      "# svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
      "# svm_classifier.fit(X_train, y_train)\n",
      "# y_pred = svm_classifier.predict(X_test)\n",
      "# accuracy = accuracy_score(y_test, y_pred)\n",
      "# print(f\"Accuracy: {accuracy}\")\n",
      "# classification_rep = classification_report(y_test, y_pred)\n",
      "# print(\"Classification Report:\\n\", classification_rep)\n",
      "\n",
      "#deep learning\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import tensorflow as tf\n",
      "from sklearn.model_selection import train_test_split\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers\n",
      "model = keras.Sequential([     #whixh allows us to build model layer by layer\n",
      "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
      "    layers.Dense(32, activation='relu'),\n",
      "    layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
      "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
      "print(f\"Test Accuracy: {test_accuracy}\")\n",
      "y_pred = model.predict(X_test)\n",
      "x1=pd.read_csv(\"credit_record.csv\")\n",
      "y1=pd.read_csv(\"application_record.csv\")\n",
      "merged_data2 = x1.merge(y1, on='ID', how='inner')\n",
      "\n",
      "\n",
      "import seaborn as sns\n",
      "# Example 8: Duration vs. Target\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.boxplot(data=merged_data, x='target', y='DURATION')\n",
      "plt.title('Duration vs. Target')\n",
      "plt.xlabel('Target')\n",
      "plt.ylabel('Duration')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Example 9: Income vs. Gender\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.boxplot(data=merged_data, x='CODE_GENDER', y='AMT_INCOME_TOTAL')\n",
      "plt.title('Income vs. Gender')\n",
      "plt.xlabel('Gender')\n",
      "plt.ylabel('Income')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Example 10: Family Members vs. Income\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.scatterplot(data=merged_data, x='CNT_FAM_MEMBERS', y='AMT_INCOME_TOTAL', hue='target')\n",
      "plt.title('Family Members vs. Income')\n",
      "plt.xlabel('Family Members')\n",
      "plt.ylabel('Income')\n",
      "plt.legend(title='Target', loc='upper right', labels=['Rejected', 'Approved'])\n",
      "plt.show()\n",
      "\n",
      "!pip install xgboost\n",
      "from xgboost import XGBClassifier\n",
      "xgb_classifier = XGBClassifier()\n",
      "xgb_classifier.fit(X_train, y_train)\n",
      "y_pred_xgb = xgb_classifier.predict(X_test)\n",
      "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
      "print(f\"XGBoost Accuracy: {xgb_accuracy}\")\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score, mean_squared_error\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "\n",
      "# Load your dataset (replace 'data.csv' with your dataset file)\n",
      "\n",
      "# Split the dataset into features (X) and target variable (y)\n",
      "X = merged_data.drop('target', axis=1)\n",
      "y = merged_data['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Standardize features (only for some models)\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "\n",
      "# Logistic Regression\n",
      "logistic_regression = LogisticRegression()\n",
      "logistic_regression.fit(X_train_scaled, y_train)\n",
      "y_pred_lr = logistic_regression.predict(X_test_scaled)\n",
      "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
      "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
      "\n",
      "# Random Forest\n",
      "random_forest = RandomForestClassifier()\n",
      "random_forest.fit(X_train, y_train)\n",
      "y_pred_rf = random_forest.predict(X_test)\n",
      "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
      "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
      "\n",
      "# Decision Tree\n",
      "decision_tree = DecisionTreeClassifier()\n",
      "decision_tree.fit(X_train, y_train)\n",
      "y_pred_dt = decision_tree.predict(X_test)\n",
      "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
      "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
      "\n",
      "# Deep Learning with TensorFlow/Keras\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
      "    keras.layers.Dense(32, activation='relu'),\n",
      "    keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "model.fit(X_train_scaled, y_train, epochs=10, batch_size=64, verbose=1)\n",
      "\n",
      "y_pred_dl = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
      "dl_accuracy = accuracy_score(y_test, y_pred_dl)\n",
      "print(f\"Deep Learning Accuracy: {dl_accuracy}\")\n",
      "\n",
      "# Mean Squared Error for Deep Learning\n",
      "mse_dl = mean_squared_error(y_test, y_pred_dl)\n",
      "print(f\"Mean Squared Error for Deep Learning: {mse_dl}\")\n",
      "\n",
      "! pip install torch\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "# Define a custom neural network\n",
      "class CustomNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(CustomNN, self).__init__()\n",
      "        self.fc1 = nn.Linear(X_train_scaled.shape[1], 64)\n",
      "        self.fc2 = nn.Linear(64, 32)\n",
      "        self.fc3 = nn.Linear(32, 1)\n",
      "        self.sigmoid = nn.Sigmoid()\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = torch.relu(self.fc2(x))\n",
      "        x = self.sigmoid(self.fc3(x))\n",
      "        return x\n",
      "\n",
      "# Create an instance of the custom neural network\n",
      "model = CustomNN()\n",
      "criterion = nn.BCELoss()\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# Training loop and evaluation\n",
      "num_epochs = 10\n",
      "for epoch in range(num_epochs):\n",
      "    model.train()\n",
      "    optimizer.zero_grad()\n",
      "    outputs = model(torch.FloatTensor(X_train_scaled))\n",
      "    loss = criterion(outputs, torch.FloatTensor(y_train.values).view(-1, 1))\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "model.eval()\n",
      "y_pred_nn = (model(torch.FloatTensor(X_test_scaled)).detach().numpy() > 0.5).astype(int)\n",
      "nn_accuracy = accuracy_score(y_test, y_pred_nn)\n",
      "print(f\"Neural Network Accuracy: {nn_accuracy}\")\n",
      "\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "param_dist = {\n",
      "    'n_estimators': [100, 200, 300, 400],\n",
      "    'max_depth': [None, 10, 20, 30],\n",
      "    'min_samples_split': [2, 5, 10],\n",
      "    'min_samples_leaf': [1, 2, 4],\n",
      "    'bootstrap': [True, False]\n",
      "}\n",
      "\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "\n",
      "random_search = RandomizedSearchCV(\n",
      "    rf, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1, random_state=42)\n",
      "\n",
      "random_search.fit(X_train, y_train)\n",
      "\n",
      "best_params = random_search.best_params_\n",
      "\n",
      "best_estimator = random_search.best_estimator_\n",
      "\n",
      "accuracy = best_estimator.score(X_test, y_test)\n",
      "\n",
      "print(\"Best Hyperparameters:\", best_params)\n",
      "print(\"Accuracy on Test Data:\", accuracy)\n",
      "\n",
      "x\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "import numpy as np\n",
      "\n",
      "X_train = np.hstack((X_train, np.zeros((X_train.shape[0], 1))))\n",
      "X_test = np.hstack((X_test, np.zeros((X_test.shape[0], 1))))\n",
      "\n",
      "\n",
      "model = keras.Sequential([\n",
      "    keras.layers.Dense(128, activation='relu', input_shape=(24,), kernel_initializer='glorot_normal'),\n",
      "    keras.layers.Dropout(0.2),\n",
      "    keras.layers.Dense(64, activation='relu', kernel_initializer='glorot_normal'),\n",
      "    keras.layers.Dropout(0.2),\n",
      "    keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
      "\n",
      "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
      "print(f'Test accuracy: {test_acc}')\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "# Support Vector Machine\n",
      "svm = SVC(kernel='linear')\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "y_pred_svm = svm.predict(X_test_scaled)\n",
      "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
      "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "# Gradient Boosting Machine\n",
      "gbm = GradientBoostingClassifier()\n",
      "gbm.fit(X_train, y_train)\n",
      "y_pred_gbm = gbm.predict(X_test)\n",
      "gbm_accuracy = accuracy_score(y_test, y_pred_gbm)\n",
      "print(f\"Gradient Boosting Accuracy: {gbm_accuracy}\")\n",
      "\n",
      "# Support Vector Machine\n",
      "svm = SVC(kernel='linear')\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "y_pred_svm = svm.predict(X_test_scaled)\n",
      "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
      "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
      "\n",
      "# Gradient Boosting Machine\n",
      "gbm = GradientBoostingClassifier()\n",
      "gbm.fit(X_train, y_train)\n",
      "y_pred_gbm = gbm.predict(X_test)\n",
      "gbm_accuracy = accuracy_score(y_test, y_pred_gbm)\n",
      "print(f\"Gradient Boosting Accuracy: {gbm_accuracy}\")\n",
      "\n",
      "# Support Vector Machine\n",
      "svm = SVC(kernel='linear')\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "y_pred_svm = svm.predict(X_test_scaled)\n",
      "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
      "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
      "\n",
      "# Gradient Boosting Machine\n",
      "gbm = GradientBoostingClassifier()\n",
      "gbm.fit(X_train, y_train)\n",
      "y_pred_gbm = gbm.predict(X_test)\n",
      "gbm_accuracy = accuracy_score(y_test, y_pred_gbm)\n",
      "print(f\"Gradient Boosting Accuracy: {gbm_accuracy}\")\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "print(y_train.value_counts())\n",
      "\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Logistic Regression\n",
      "logistic_regression.fit(X_train_scaled, y_train)\n",
      "y_pred_lr = logistic_regression.predict(X_test_scaled)\n",
      "print(classification_report(y_test, y_pred_lr))\n",
      "\n",
      "# Random Forest\n",
      "random_forest.fit(X_train, y_train)\n",
      "y_pred_rf = random_forest.predict(X_test)\n",
      "print(classification_report(y_test, y_pred_rf))\n",
      "\n",
      "# Decision Tree\n",
      "decision_tree.fit(X_train, y_train)\n",
      "y_pred_dt = decision_tree.predict(X_test)\n",
      "print(classification_report(y_test, y_pred_dt))\n",
      "\n",
      "# SVM\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "y_pred_svm = svm.predict(X_test_scaled)\n",
      "print(classification_report(y_test, y_pred_svm))\n",
      "\n",
      "# GBM\n",
      "gbm.fit(X_train, y_train)\n",
      "y_pred_gbm = gbm.predict(X_test)\n",
      "print(classification_report(y_test, y_pred_gbm))\n",
      "\n",
      "!pip install imblearn\n",
      "from imblearn.over_sampling import SMOTE\n",
      "\n",
      "smote = SMOTE(random_state=42)\n",
      "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Define the hyperparameter grid\n",
      "param_grid = {\n",
      "    'n_estimators': [100, 200, 300],\n",
      "    'max_depth': [10, 20, 30],\n",
      "    'min_samples_split': [2, 5],\n",
      "    'min_samples_leaf': [1, 2],\n",
      "}\n",
      "\n",
      "# Instantiate the grid search model\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
      "                           cv=5, n_jobs=-1, verbose=2)\n",
      "\n",
      "# Fit the grid search to the data\n",
      "grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Get the best parameters\n",
      "best_params = grid_search.best_params_\n",
      "print(best_params)\n",
      "\n",
      "best_params = grid_search.best_params_\n",
      "best_estimator = grid_search.best_estimator_\n",
      "\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "\n",
      "# Predict using the best estimator\n",
      "y_pred_best = best_estimator.predict(X_test)\n",
      "\n",
      "# Print classification report and accuracy score\n",
      "print(classification_report(y_test, y_pred_best))\n",
      "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
      "\n",
      "# Get feature importances\n",
      "importances = best_estimator.feature_importances_\n",
      "\n",
      "# Sort feature importances in descending order\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "# Rearrange feature names so they match the sorted feature importances\n",
      "names = [X.columns[i] for i in indices]\n",
      "\n",
      "# Create plot\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.title(\"Feature Importance\")\n",
      "plt.bar(range(X.shape[1]), importances[indices])\n",
      "plt.xticks(range(X.shape[1]), names, rotation=90)\n",
      "plt.show()\n",
      "\n",
      "print(classification_report(y_test, y_pred_best))\n",
      "\n",
      "from sklearn.model_selection import learning_curve\n",
      "\n",
      "train_sizes, train_scores, test_scores = learning_curve(best_estimator, X_train, y_train, cv=5)\n",
      "\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n",
      "plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-Validation Score')\n",
      "plt.title('Learning Curve')\n",
      "plt.xlabel('Training Size')\n",
      "plt.ylabel('Score')\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm = confusion_matrix(y_test, y_pred_best)\n",
      "print(cm)\n",
      "\n",
      "\n",
      "import pickle\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "pickle.dump(random_forest, open('credit_card_approval_model.pkl', 'wb'))\n",
      "\n",
      "cd D:\\minor proj\\CreaditApproval\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "# Load the saved model\n",
      "filename = 'credit_card_approval_model.pkl'\n",
      "model = pickle.load(open(filename, 'rb'))\n",
      "\n",
      "# Sample input data\n",
      "data = {\n",
      "    'CODE_GENDER': 'M',\n",
      "    'FLAG_OWN_CAR': 'Yes',\n",
      "    'FLAG_OWN_REALITY': 'Yes',\n",
      "    'CNT_CHILDREN': 2,\n",
      "    'AMT_INCOME_TOTAL': 50000,\n",
      "    'NAME_INCOME_TYPE': 'Working',\n",
      "    'NAME_EDUCATION_TYPE': 'Higher education',\n",
      "    'NAME_FAMILY_STATUS': 'Civil marriage',\n",
      "    'NAME_HOUSING_TYPE': 'Rented apartment',\n",
      "    'DAYS_BIRTH': -12005,\n",
      "    'DAYS_EMPLOYED': -4542,\n",
      "    'FLAG_MOBIL': 1,\n",
      "    'FLAG_WORK_PHONE': 1,\n",
      "    'FLAG_PHONE': 0,\n",
      "    'FLAG_EMAIL': 0,\n",
      "    'OCCUPATION_TYPE': 'Security staff',\n",
      "    'CNT_FAM_MEMBERS': 2\n",
      "}\n",
      "\n",
      "# Define mappings\n",
      "income_type_mapping = {\n",
      "    'Working': 0,\n",
      "    'Commercial associate': 1,\n",
      "    'Pensioner': 2,\n",
      "    'State servant': 3,\n",
      "    'Student': 4\n",
      "}\n",
      "\n",
      "education_mapping = {\n",
      "    'Higher education': 0,\n",
      "    'Secondary / secondary special': 1,\n",
      "    'Incomplete higher': 2,\n",
      "    'Lower secondary': 3,\n",
      "    'Academic degree': 4\n",
      "}\n",
      "\n",
      "family_status_mapping = {\n",
      "    'Civil marriage': 0,\n",
      "    'Married': 1,\n",
      "    'Single / not married': 2,\n",
      "    'Separated': 3,\n",
      "    'Widow': 4\n",
      "}\n",
      "\n",
      "housing_type_mapping = {\n",
      "    'Rented apartment': 0,\n",
      "    'House / apartment': 1,\n",
      "    'Municipal apartment': 2,\n",
      "    'With parents': 3,\n",
      "    'Co-op apartment': 4,\n",
      "    'Office apartment': 5\n",
      "}\n",
      "\n",
      "occupation_mapping = {\n",
      "    'Security staff': 0,\n",
      "    'Sales staff': 1,\n",
      "    'Accountants': 2,\n",
      "    'Laborers': 3,\n",
      "    'Managers': 4,\n",
      "    'Drivers': 5,\n",
      "    'Core staff': 6,\n",
      "    'High skill tech staff': 7,\n",
      "    'Cleaning staff': 8,\n",
      "    'Private service staff': 9,\n",
      "    'Cooking staff': 10,\n",
      "    'Low-skill Laborers': 11,\n",
      "    'Waiters/barmen staff': 12,\n",
      "    'Medicine staff': 13,\n",
      "    'Secretaries': 14,\n",
      "    'HR staff': 15,\n",
      "    'IT staff': 16\n",
      "}\n",
      "\n",
      "# Convert categorical data to numerical using the mappings\n",
      "data['CODE_GENDER'] = 1 if data['CODE_GENDER'] == 'M' else 0\n",
      "data['FLAG_OWN_CAR'] = 1 if data['FLAG_OWN_CAR'] == 'Yes' else 0\n",
      "data['FLAG_OWN_REALITY'] = 1 if data['FLAG_OWN_REALITY'] == 'Yes' else 0\n",
      "data['NAME_INCOME_TYPE'] = income_type_mapping[data['NAME_INCOME_TYPE']]\n",
      "data['NAME_EDUCATION_TYPE'] = education_mapping[data['NAME_EDUCATION_TYPE']]\n",
      "data['NAME_FAMILY_STATUS'] = family_status_mapping[data['NAME_FAMILY_STATUS']]\n",
      "data['NAME_HOUSING_TYPE'] = housing_type_mapping[data['NAME_HOUSING_TYPE']]\n",
      "data['OCCUPATION_TYPE'] = occupation_mapping[data['OCCUPATION_TYPE']]\n",
      "\n",
      "# Prepare the features with all 23 features\n",
      "features = np.array([\n",
      "    data['CODE_GENDER'], data['FLAG_OWN_CAR'], data['FLAG_OWN_REALITY'],\n",
      "    data['CNT_CHILDREN'], data['AMT_INCOME_TOTAL'], data['NAME_INCOME_TYPE'],\n",
      "    data['NAME_EDUCATION_TYPE'], data['NAME_FAMILY_STATUS'], data['NAME_HOUSING_TYPE'],\n",
      "    data['DAYS_BIRTH'], data['DAYS_EMPLOYED'], data['FLAG_MOBIL'], data['FLAG_WORK_PHONE'],\n",
      "    data['FLAG_PHONE'], data['FLAG_EMAIL'], data['OCCUPATION_TYPE'], data['CNT_FAM_MEMBERS'],\n",
      "    0, 0, 0, 0, 0, 0  # Add placeholders for the missing features\n",
      "]).reshape(1, -1)\n",
      "\n",
      "# Predict\n",
      "prediction = model.predict(features)\n",
      "prediction_probability = model.predict_proba(features)\n",
      "\n",
      "print(f\"Predicted Class: {prediction[0]}\")\n",
      "print(f\"Predicted Probabilities: {prediction_probability[0]}\")\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import joblib\n",
      "import os\n",
      "\n",
      "# Assuming X_cc_train_prep and y_cc_train_prep are your training data\n",
      "\n",
      "# Initialize Random Forest classifier\n",
      "random_forest = RandomForestClassifier()\n",
      "\n",
      "# Train the model\n",
      "random_forest.fit(X_cc_train_prep, y_cc_train_prep)\n",
      "\n",
      "# Define the directory and file path\n",
      "directory = 'saved_models/random_forest/'\n",
      "file_path = os.path.join(directory, 'random_forest_model.pkl')\n",
      "\n",
      "# Create the directory if it doesn't exist\n",
      "if not os.path.exists(directory):\n",
      "    os.makedirs(directory)\n",
      "\n",
      "# Save the model\n",
      "joblib.dump(random_forest, file_path)\n",
      "\n",
      "print(\"Random Forest model trained and saved successfully!\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Credit Score Distribution: We observe that the credit scores of approved applicants have a higher average and tighter distribution compared to rejected applicants. This suggests that credit scores strongly influence approval decisions.\n",
      "\n",
      "Income vs. Approval: EDA reveals that applicants with higher incomes are more likely to be approved for credit, indicating that income is a significant factor in approval decisions.\n",
      "\n",
      "Age Distribution: You notice that the age of applicants who are rejected tends to be younger on average than those who are approved. This could imply age-related factors influencing approvals.\n",
      "\n",
      "Loan Amount Requested: EDA shows that the requested loan amounts for approved applications have a wide range, while rejected applications tend to request higher loan amounts. This may indicate a preference for lower-risk loans.\n",
      "\n",
      "Employment Status: Analysis reveals that employed applicants are more likely to be approved, while unemployed or self-employed applicants face more rejections. Employment status appears to be a strong predictor.\n",
      "\n",
      "Gender Bias: EDA indicates that there may be a gender bias in approvals, with one gender being approved at a significantly higher rate. Further investigation is needed to understand the reasons behind this bias.\n",
      "\n",
      "Credit History Length: Applicants with longer credit histories are more likely to be approved. This suggests that the length of a credit history is an essential factor in credit decisions.\n",
      "\n",
      "Region Analysis: You find that approval rates vary by region, with certain geographic areas having higher approval rates. This could be due to economic conditions or local lending practices.\n",
      "\n",
      "Monthly Debt-to-Income Ratio: EDA shows that applicants with a high monthly debt-to-income ratio are more likely to be rejected. This highlights the importance of debt management in credit decisions.\n",
      "\n",
      "Default Rates Over Time: When analyzing historical data, you observe that default rates have been increasing over the past few years, suggesting a need for tighter risk assessment.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the Jupyter Notebook file\n",
    "file_path = \"D:\\CreaditApproval\\CreaditApproval\\creditcard project final.ipynb\"  # Replace 'your_notebook.ipynb' with your actual notebook file path\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Initialize an empty list to store cell content\n",
    "all_cells_content = []\n",
    "\n",
    "# Iterate through each cell in the notebook\n",
    "for cell in notebook_content.cells:\n",
    "    # Check cell type: 'code', 'markdown', 'raw'\n",
    "    if cell.cell_type == 'code':\n",
    "        # Extract code from code cells\n",
    "        all_cells_content.append(cell.source)\n",
    "    elif cell.cell_type == 'markdown':\n",
    "        # Extract markdown text from markdown cells\n",
    "        all_cells_content.append(cell.source)\n",
    "    elif cell.cell_type == 'raw':\n",
    "        # Extract raw text from raw cells\n",
    "        all_cells_content.append(cell.source)\n",
    "\n",
    "# Join all cell content into a single string\n",
    "all_cells_content_str = '\\n'.join(all_cells_content)\n",
    "\n",
    "# Print or save the concatenated cell content\n",
    "print(all_cells_content_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4678466-937b-463c-9d0a-ca23a68f13ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
